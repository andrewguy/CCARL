{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import URLError\n",
    "import os\n",
    "import json\n",
    "from os import path\n",
    "\n",
    "DATA_DIR = \"../../Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    '''Remove a prefix from a string, returning a new string.'''\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "def remove_suffix(text, suffix):\n",
    "    '''Remove a suffix from a string, returning a new string.'''\n",
    "    if text.endswith(suffix):\n",
    "        return text[:-len(suffix)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(DATA_DIR, 'Initial_CFG_HTML_Query_20180426.html'), encoding='windows-1252') as f:\n",
    "    html_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = html.fromstring(html_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = tree.find_class(\"tips\")[0].find_class(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some HTML scraping to pull out key parameters and urls from saved HTML search.\n",
    "data_list = []\n",
    "for element in results_table:\n",
    "    sample = element.find_class('webSiteBody')[0][0].text\n",
    "    try:\n",
    "        sample_html = element.find_class('webSiteBody')[0][0].attrib['href']\n",
    "        sample_html = remove_prefix(sample_html, \"javascript:openWindow('\")\n",
    "        sample_html = remove_suffix(sample_html, \"')\")\n",
    "    except KeyError:\n",
    "        try:\n",
    "            sample_html = element.find_class('webSiteBody')[0][-1].attrib['href']\n",
    "            sample_html = remove_prefix(sample_html, \"javascript:openWindow('\")\n",
    "            sample_html = remove_suffix(sample_html, \"')\")\n",
    "        except:\n",
    "            sample_html = None\n",
    "    species = element.find_class('webSiteBody')[1].text\n",
    "    protein_family = element.find_class('webSiteBody')[2].text\n",
    "    investigator = element.find_class('webSiteBody')[3].text_content()\n",
    "    experiment = element.find_class('websiteBodyLight')[0].text\n",
    "    try:\n",
    "        data_url = element.find_class('webSiteBody')[5][0][0][0][1][0].attrib['href']\n",
    "        data_url = remove_prefix(data_url, \"javascript:openWindow('\")\n",
    "        data_url = remove_suffix(data_url, \"')\")\n",
    "    except IndexError:\n",
    "        data_url = None\n",
    "    for a_href in element.iter(\"a\"):\n",
    "        if 'primscreen' in a_href.attrib['href']:\n",
    "            primscreen_id = a_href.attrib['href'].split('primscreen_')[1][0:-2]\n",
    "    data_list.append({'sample': sample, 'sample_html': sample_html, 'species': species, 'protein_family': protein_family,\n",
    "                      'experiment': experiment, 'data_url': data_url, 'data_file': None, 'investigator': investigator, 'primscreen_id': primscreen_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all data files and save them locally\n",
    "errors = []\n",
    "for i, datum in enumerate(data_list):\n",
    "    if not datum['data_url']:\n",
    "        continue\n",
    "    try:\n",
    "        response = urlretrieve(datum['data_url'])\n",
    "    except URLError:\n",
    "        # Can come back and fix any errors if we need to.\n",
    "        errors.append(i)\n",
    "        continue\n",
    "    filename = response[1].get_filename()\n",
    "    os.rename(response[0], path.join(DATA_DIR, 'CFG_Data_Files', filename))\n",
    "    datum['data_file'] = filename\n",
    "\n",
    "# There shouldn't be any errors here, but perhaps if there are connection issues there might be some.\n",
    "# Can always rerun these if we need to.\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list of all data entries and associated file names.\n",
    "with open(path.join(DATA_DIR,'Data_Index.json'), 'w') as f:\n",
    "    json.dump(data_list, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
